[
  {
    "path": "posts/2020-12-04-description-of-the-dataset/",
    "title": "Description of the dataset",
    "description": "Core features of the data, variable transformations, and data splitting process.",
    "author": [
      {
        "name": "Jim - Claire - Thuy",
        "url": "https://www.uoregon.edu/"
      }
    ],
    "date": "2020-12-04",
    "categories": [],
    "contents": "\nDescription of the data\nThe original training dataset included 39 variables. We randomly sampled 7% of this dataset to reduce the total to 13,260 observations. To expand the number of predictors in our models, the following steps were taken:\nFRL (6 variables) and student_count (2 variables) imported and joined together by ncessch variable.\nTwo variables created in the FRL dataset:\nproportion of students who qualify for free and reduced lunch\nproportion of students who qualify for reduced price lunch\n\nEthnicities data imported\nThe original 7% sample of the training data was joined with the FRL and Ethnicities datasets to increase the total number of variables to 55 across `3,263 observations.\nScore Variable\nWe’re interested in predicting the students’ score based on their attributes. The range of the outcome variable is as follows:\nSummary\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1821    2420    2497    2498    2575    3550 \n\nDistribution\nThe distribution of of the 7% random sample of scores is a normal distribution. Additionally, there are no apparent skew or outliers.\n\n\n\nDescription of the remaining 54 variables\nID Variables\nVariable ID\nVariable Description\nid\nStudent identifier\nattnd_dist_inst_id\nInstitution identifier for the Attending District\nattnd_schl_inst_id\nInstitution identifier for the Attending School\npartic_dist_inst_id\nInstitution identifier for the Attending District\npartic_schl_inst_id\nInstitution identifier for the Attending School\nncessch\nSchool id number\nCategorical Predictor Variables\nVariable ID\nVariable Description\ngndr\nStudent gender\nethnic_cd\nStudent ethnicity\nenrl_grd\nStudent grade level\ncalc_admn_cd\nCode describing special circumstances affecting test administration\ntst_bnch\nBenchmark level of the administered test\ntst_dt\nTest date\nmigrant_ed_fg\nIndicates student participation in a program designed to assure that migratory children receive full and appropriate opportunity to meet the state academic content and student academic achievement standards\nind_ed_fg\nIndicates student participation in a program designed to meet the unique educational and culturally related academic needs of American Indians\nsp_ed_fg\nIndicates student participation in an Individualized Education Plan (IEP/IFSP)\ntag_ed_fg\nIndicates student participation in a Talented and Gifted program\necon_dsvntg\nIndicates student eligibility for a Free or Reduced Lunch program\nayp_lep\nIndicates a student who received services or was eligible to receive services in a Limited English Proficient program\nstay_in_dist\nIndicates that the student has been enrolled for more than 50% of the days in the school year as of the first school day in May at the district where the student is resident on the first school day in May\nstay_in_schl\nIndicates that the student has been enrolled for more than 50% of the days in the school year as of the first school day in May at the school where the student is resident on the first school day in May\ndist_sped\nIndicates that the student was enrolled in a district special education program during the school year and received general education classroom instruction for less than 40% of the time as of the first school day in May\ntrgt_assist_fg\nFlag indicating the record is included in Title 1 Targeted Assistance for the Adequate Yearly Progress (AYP) school performance calculations\nayp_dist_partic\nFlag indicating the record is included in the denominator of Adequate Yearly Progress (AYP) district participation calculations\nayp_schl_partic\nFlag indicating the record is included in the denominator of Adequate Yearly Progress (AYP) school participation calculations\nayp_dist_prfrm\nFlag indicating the record is included in the denominator of Adequate Yearly Progress (AYP) district performance calculations\nayp_schl_prfrm\nFlag indicating the record is included in the denominator of Adequate Yearly Progress (AYP) school performance calculations\nrc_dist_partic\nFlag indicating the record is included in the denominator of Report Card (RC) district participation calculations\nrc_schl_partic\nFlag indicating the record is included in the denominator of Report Card (RC) school participation calculations\nrc_dist_prfrm\nFlag indicating the record is included in the denominator of Report Card (RC) district performance calculations\nrc_schl_prfrm\nFlag indicating the record is included in the denominator of Report Card (RC) school participation calculations\nlang_cd\nTest language\ntst_atmpt_fg\nCode describing whether the test was attempted\ngrp_rpt_dist_partic\nFlag indicating the record is included in the denominator of Group Report district participation calculations\ngrp_rpt_schl_inst_id\nFlag indicating the record is included in the denominator of Group Report school participation calculations\ngrp_rpt_dist_prfrm\nFlag indicating the record is included in the denominator of Group Report district performance calculations\ngrp_rpt_schl_prfrm\nFlag indicating the record is included in the denominator of Group Report school participation calculations\nsch_name\nSchool name\nGender Distribution\n\n\n\nEthnicity Distribution\n\n\n\nContinuous Predictor Variables\nVariable ID\nVariable Description\nlat\nSchool latitude\nlon\nschool longitude\np_american_indian_alaska_native\nPercentage of students identified as American Indian and/or Alaska Native\np_asian\nPercentage of students identified as Asian\np_native_hawaiin_pacific_islander\nPercentage of students identified as Native Hawaiin and/or Pacific Islander\np_black_african_american\nPercentage of students identified as black and/or African American\np_hispanic_latino\nPercentage of students identified as Hispanic and/or Latino\np_white\nPercentage of students identified as white\np_multiracial\nPercentage of students identified as multiracial\nfree_lunch_qualified\nTotal number of students who qualify for free lunch\nreduced_price_lunch_qualified\nTotal number of students who qualify for reduced price lunch\nmissing\n\nnot_applicable\n\nno_category_codes\n\nn\nTotal number of students enrolled in the school\nprop_frl\nProportion of students who qualify for free lunch\nprop_reduced_lunch\nProportion of students who qualify for reduced price lunch\nDistribution of Ethnicities\nAmerican Indian/Alaskan Native race, non-Hispanic\n\n\n\nAsian, non-Hispanic\n\n\n\nBlack/African American, non-Hispanic\n\n\n\nHispanic\n\n\n\nMulti-racial, non-Hispanic\n\n\n\nPacific Islander race, non-Hispanic\n\n\n\nWhite\n\n\n\nDistribution of Free and Reduced Lunch Status\nDistribution of the proportion of students who qualify for free lunch\n\n\n\nDistribution of the proportion of students who qualify for reduced price lunch\n\n\n\n\n       id             gndr            ethnic_cd        \n Min.   :    22   Length:13263       Length:13263      \n 1st Qu.: 62804   Class :character   Class :character  \n Median :125333   Mode  :character   Mode  :character  \n Mean   :125772                                        \n 3rd Qu.:188416                                        \n Max.   :252568                                        \n                                                       \n attnd_dist_inst_id attnd_schl_inst_id    enrl_grd     calc_admn_cd  \n Min.   :1894       Min.   :   1       Min.   :3.000   Mode:logical  \n 1st Qu.:2039       1st Qu.: 506       1st Qu.:4.000   NA's:13263    \n Median :2142       Median : 942       Median :5.000                 \n Mean   :2123       Mean   :1373       Mean   :5.471                 \n 3rd Qu.:2190       3rd Qu.:1300       3rd Qu.:7.000                 \n Max.   :4131       Max.   :5392       Max.   :8.000                 \n                                                                     \n   tst_bnch            tst_dt          migrant_ed_fg     \n Length:13263       Length:13263       Length:13263      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n  ind_ed_fg           sp_ed_fg          tag_ed_fg        \n Length:13263       Length:13263       Length:13263      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n econ_dsvntg          ayp_lep          stay_in_dist      \n Length:13263       Length:13263       Length:13263      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n stay_in_schl        dist_sped         trgt_assist_fg    \n Length:13263       Length:13263       Length:13263      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n ayp_dist_partic    ayp_schl_partic    ayp_dist_prfrm    \n Length:13263       Length:13263       Length:13263      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n ayp_schl_prfrm     rc_dist_partic     rc_schl_partic    \n Length:13263       Length:13263       Length:13263      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n rc_dist_prfrm      rc_schl_prfrm      partic_dist_inst_id\n Length:13263       Length:13263       Min.   :1894       \n Class :character   Class :character   1st Qu.:2041       \n Mode  :character   Mode  :character   Median :2142       \n                                       Mean   :2124       \n                                       3rd Qu.:2190       \n                                       Max.   :4131       \n                                       NA's   :44         \n partic_schl_inst_id   lang_cd          tst_atmpt_fg      \n Min.   :   1        Length:13263       Length:13263      \n 1st Qu.: 506        Class :character   Class :character  \n Median : 942        Mode  :character   Mode  :character  \n Mean   :1375                                             \n 3rd Qu.:1300                                             \n Max.   :5392                                             \n NA's   :44                                               \n grp_rpt_dist_partic grp_rpt_schl_partic grp_rpt_dist_prfrm\n Length:13263        Length:13263        Length:13263      \n Class :character    Class :character    Class :character  \n Mode  :character    Mode  :character    Mode  :character  \n                                                           \n                                                           \n                                                           \n                                                           \n grp_rpt_schl_prfrm     score         ncessch         \n Length:13263       Min.   :1821   Min.   :4.100e+11  \n Class :character   1st Qu.:2420   1st Qu.:4.103e+11  \n Mode  :character   Median :2497   Median :4.108e+11  \n                    Mean   :2498   Mean   :4.107e+11  \n                    3rd Qu.:2575   3rd Qu.:4.111e+11  \n                    Max.   :3550   Max.   :4.114e+11  \n                                   NA's   :202        \n      lat             lon           sch_name        \n Min.   :42.01   Min.   :-124.5   Length:13263      \n 1st Qu.:44.25   1st Qu.:-123.0   Class :character  \n Median :45.27   Median :-122.8   Mode  :character  \n Mean   :44.79   Mean   :-122.5                     \n 3rd Qu.:45.50   3rd Qu.:-122.5                     \n Max.   :46.18   Max.   :-116.9                     \n NA's   :218     NA's   :218                        \n p_american_indian_alaska_native    p_asian       \n Min.   :0.00000                 Min.   :0.00000  \n 1st Qu.:0.00265                 1st Qu.:0.00551  \n Median :0.00583                 Median :0.01375  \n Mean   :0.01227                 Mean   :0.03919  \n 3rd Qu.:0.01174                 3rd Qu.:0.03846  \n Max.   :0.88284                 Max.   :0.61931  \n NA's   :10                      NA's   :10       \n p_native_hawaiian_pacific_islander p_black_african_american\n Min.   :0.000000                   Min.   :0.00000         \n 1st Qu.:0.000000                   1st Qu.:0.00371         \n Median :0.003540                   Median :0.00926         \n Mean   :0.007837                   Mean   :0.02188         \n 3rd Qu.:0.009500                   3rd Qu.:0.02116         \n Max.   :0.083800                   Max.   :0.49846         \n NA's   :10                         NA's   :10              \n p_hispanic_latino    p_white       p_multiracial    \n Min.   :0.0000    Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.1008    1st Qu.:0.4533   1st Qu.:0.04474  \n Median :0.1850    Median :0.6538   Median :0.06673  \n Mean   :0.2439    Mean   :0.6069   Mean   :0.06680  \n 3rd Qu.:0.3299    3rd Qu.:0.7734   3rd Qu.:0.08472  \n Max.   :1.0000    Max.   :1.0000   Max.   :0.35088  \n NA's   :10        NA's   :10       NA's   :10       \n free_lunch_qualified reduced_price_lunch_qualified    missing   \n Min.   :  0.0        Min.   :  0.00                Min.   :0    \n 1st Qu.:122.0        1st Qu.: 22.00                1st Qu.:0    \n Median :211.0        Median : 36.00                Median :0    \n Mean   :228.6        Mean   : 39.57                Mean   :0    \n 3rd Qu.:309.0        3rd Qu.: 52.00                3rd Qu.:0    \n Max.   :813.0        Max.   :132.00                Max.   :0    \n NA's   :202          NA's   :202                   NA's   :202  \n not_applicable no_category_codes       n             prop_frl     \n Min.   :0      Min.   :  0.0     Min.   :  12.0   Min.   :0.0000  \n 1st Qu.:0      1st Qu.:152.0     1st Qu.: 420.0   1st Qu.:0.1916  \n Median :0      Median :248.0     Median : 599.0   Median :0.2921  \n Mean   :0      Mean   :268.2     Mean   : 807.3   Mean   :0.3562  \n 3rd Qu.:0      3rd Qu.:357.0     3rd Qu.:1165.0   3rd Qu.:0.4918  \n Max.   :0      Max.   :920.0     Max.   :3144.0   Max.   :4.3823  \n NA's   :202    NA's   :202       NA's   :202      NA's   :202     \n prop_reduced_lunch\n Min.   :0.00000   \n 1st Qu.:0.03633   \n Median :0.05385   \n Mean   :0.06072   \n 3rd Qu.:0.07481   \n Max.   :1.20833   \n NA's   :202       \n\nSplit and Resample\nThe code chunk below details on the training data was split and resampled with k-fold cross validation to generate all three models.\n\n\n\nPreprocess\nWe processed the varibales in the dataset through the following steps:\nAll id variables were assigned to “id” role, which are not used as predictors.\nAll nominal variables, except the outcome, with a missing value in a factor level were assigned to “unknown” level.\nAll numeric variables with a missing value were imputed by its median value.\nPredictor variables with nearly no variation were removed.\nAll nominal variables, except id variables and the outcome variable, were dummy coded.\nFollowing the implementation of dummy coding, all nominal variables with nearly no variation were removed.\nThe final processed data generated:\n6 id variables\n1 outcome variable\n48 predictor variables\n\nData Recipe\n\nInputs:\n\n      role #variables\n   id vars          6\n   outcome          1\n predictor         48\n\nTraining data contained 9949 data points and 9949 incomplete rows. \n\nOperations:\n\nVariable mutation for tst_dt [trained]\nNovel factor level assignment for gndr, ethnic_cd, ... [trained]\nUnknown factor level assignment for gndr, ethnic_cd, ... [trained]\nSparse, unbalanced variable filter removed calc_admn_cd, ... [trained]\nMedian Imputation for enrl_grd, tst_dt, lat, ... [trained]\nDummy variables from gndr, ethnic_cd, tst_bnch, sp_ed_fg, ... [trained]\nSparse, unbalanced variable filter removed gndr_new, ... [trained]\nInteractions with lat:lon [trained]\n# A tibble: 9,949 x 37\n       id attnd_dist_inst… attnd_schl_inst… enrl_grd tst_dt\n    <dbl>            <dbl>            <dbl>    <dbl>  <dbl>\n 1 188408             1970              219        3 1.52e9\n 2 226880             2252             1208        4 1.53e9\n 3   3436             2243             1176        5 1.53e9\n 4  48828             2043              390        4 1.52e9\n 5 252482             2087              574        3 1.53e9\n 6 226888             1922             1287        8 1.53e9\n 7  70637             2197             1012        6 1.53e9\n 8 170356             2239             4641        4 1.53e9\n 9 158918             2142              763        3 1.53e9\n10 185543             2142              754        4 1.53e9\n# … with 9,939 more rows, and 32 more variables:\n#   partic_dist_inst_id <dbl>, partic_schl_inst_id <dbl>,\n#   ncessch <dbl>, lat <dbl>, lon <dbl>,\n#   p_american_indian_alaska_native <dbl>, p_asian <dbl>,\n#   p_hispanic_latino <dbl>, p_white <dbl>, p_multiracial <dbl>,\n#   free_lunch_qualified <dbl>, reduced_price_lunch_qualified <dbl>,\n#   no_category_codes <dbl>, n <int>, prop_frl <dbl>,\n#   prop_reduced_lunch <dbl>, score <dbl>, gndr_M <dbl>,\n#   ethnic_cd_H <dbl>, ethnic_cd_M <dbl>, ethnic_cd_W <dbl>,\n#   tst_bnch_X2B <dbl>, tst_bnch_X3B <dbl>, tst_bnch_G4 <dbl>,\n#   tst_bnch_G6 <dbl>, tst_bnch_G7 <dbl>, sp_ed_fg_Y <dbl>,\n#   tag_ed_fg_Y <dbl>, econ_dsvntg_Y <dbl>, ayp_lep_F <dbl>,\n#   ayp_lep_unknown <dbl>, lat_x_lon <dbl>\n\n\n\n\n",
    "preview": "posts/2020-12-04-description-of-the-dataset/description-of-the-dataset_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-12-11T20:32:45-08:00",
    "input_file": "description-of-the-dataset.utf8.md"
  },
  {
    "path": "posts/2020-12-04-three-models/",
    "title": "Three models",
    "description": "Three models: fitting procedures and the results of model evaluation.",
    "author": [
      {
        "name": "Jim - Claire - Thuy",
        "url": "https://www.uoregon.edu/"
      }
    ],
    "date": "2020-12-04",
    "categories": [],
    "contents": "\nData\nSplit and Resample\n\n\n\nRecipe\n\nData Recipe\n\nInputs:\n\n      role #variables\n   id vars          6\n   outcome          1\n predictor         48\n\nTraining data contained 9949 data points and 9949 incomplete rows. \n\nOperations:\n\nVariable mutation for tst_dt [trained]\nNovel factor level assignment for gndr, ethnic_cd, ... [trained]\nUnknown factor level assignment for gndr, ethnic_cd, ... [trained]\nSparse, unbalanced variable filter removed calc_admn_cd, ... [trained]\nMedian Imputation for enrl_grd, tst_dt, lat, ... [trained]\nDummy variables from gndr, ethnic_cd, tst_bnch, sp_ed_fg, ... [trained]\nSparse, unbalanced variable filter removed gndr_new, ... [trained]\nInteractions with lat:lon [trained]\n# A tibble: 9,949 x 37\n       id attnd_dist_inst… attnd_schl_inst… enrl_grd tst_dt\n    <dbl>            <dbl>            <dbl>    <dbl>  <dbl>\n 1 188408             1970              219        3 1.52e9\n 2 226880             2252             1208        4 1.53e9\n 3   3436             2243             1176        5 1.53e9\n 4  48828             2043              390        4 1.52e9\n 5 252482             2087              574        3 1.53e9\n 6 226888             1922             1287        8 1.53e9\n 7  70637             2197             1012        6 1.53e9\n 8 170356             2239             4641        4 1.53e9\n 9 158918             2142              763        3 1.53e9\n10 185543             2142              754        4 1.53e9\n# … with 9,939 more rows, and 32 more variables:\n#   partic_dist_inst_id <dbl>, partic_schl_inst_id <dbl>,\n#   ncessch <dbl>, lat <dbl>, lon <dbl>,\n#   p_american_indian_alaska_native <dbl>, p_asian <dbl>,\n#   p_hispanic_latino <dbl>, p_white <dbl>, p_multiracial <dbl>,\n#   free_lunch_qualified <dbl>, reduced_price_lunch_qualified <dbl>,\n#   no_category_codes <dbl>, n <int>, prop_frl <dbl>,\n#   prop_reduced_lunch <dbl>, score <dbl>, gndr_M <dbl>,\n#   ethnic_cd_H <dbl>, ethnic_cd_M <dbl>, ethnic_cd_W <dbl>,\n#   tst_bnch_X2B <dbl>, tst_bnch_X3B <dbl>, tst_bnch_G4 <dbl>,\n#   tst_bnch_G6 <dbl>, tst_bnch_G7 <dbl>, sp_ed_fg_Y <dbl>,\n#   tag_ed_fg_Y <dbl>, econ_dsvntg_Y <dbl>, ayp_lep_F <dbl>,\n#   ayp_lep_unknown <dbl>, lat_x_lon <dbl>\n\nModel 1: Linear Model\nRationale for Selecting a Linear Model\nLinear models assume a normal distribution within the data it is applied to. As demonstrated in the histogram, the score variable displays a normal distribution, which suggests a linear model can be fit to the data with both low variance and low bias.\n\n\n\nModel Construction\nThe three linear models presented in the course represent penalized regression models, which shrink the coefficients towards zero in order to reduce the model’s overall variance.\nRidge Regression Model: Shrinks coefficients of correlated predictors toward each other, which is beneficial when the purpose of the model is to keep all predictors.\nLasso Regression Model: Selects one predictor to model the outcome while ignoring other predictors, which is beneficial to identifying the largest and most consistent predictor in data with may predictors.\nElastic Net Regression Model: Combines the two types of penalties, which is better suited for multicollinearity.\nInstead of selecting one of the specific penalties to run our linear model, we opted to use cross-validation to find the optimal tuning parameters with a grid search. We specifically tuned the penalty and mixture parameters of the linear model. Our grid search utilized a regular grid with 10 possible values for the penalty and 5 possible values for the mixture, which produced 50 models per fold, resulting in a total of 500 models.\n\n74.062 sec elapsed\n# A tibble: 100 x 8\n       penalty mixture .metric .estimator   mean     n std_err .config\n         <dbl>   <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>  \n 1    1.00e-10       0 rmse    standard   90.4      10 0.655   Model01\n 2    1.00e-10       0 rsq     standard    0.402    10 0.00692 Model01\n 3    1.29e- 9       0 rmse    standard   90.4      10 0.655   Model02\n 4    1.29e- 9       0 rsq     standard    0.402    10 0.00692 Model02\n 5    1.67e- 8       0 rmse    standard   90.4      10 0.655   Model03\n 6    1.67e- 8       0 rsq     standard    0.402    10 0.00692 Model03\n 7    2.15e- 7       0 rmse    standard   90.4      10 0.655   Model04\n 8    2.15e- 7       0 rsq     standard    0.402    10 0.00692 Model04\n 9    2.78e- 6       0 rmse    standard   90.4      10 0.655   Model05\n10    2.78e- 6       0 rsq     standard    0.402    10 0.00692 Model05\n# … with 90 more rows\n# A tibble: 1 x 8\n       penalty mixture .metric .estimator  mean     n std_err .config\n         <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1 0.0000000001     0.5 rmse    standard    90.4    10   0.656 Model21\n\nEvaluation of the Model\nThe specific metric used to evaluate model performance was the RMSE. Utilzing the show_best() function, we isolated the best performing model, which obtained a RMSE value of approximately 90.5. As we randomly sampled 7% of the data, consecutive runs of the model generated RMSE values ranging from 90.4 - 90.6. Of note, initial iterations of the linear regression model were run exclusively on the combined FRL and training data, which resulted in a RMSE value of ~96.0. Expanding the data set to include the ethnicities and ode_schools and adding the interaction of lat and lon to the recipe decreased the RMSE to ~90.5. The strength of the linear model is likely a direct result of the normal distribution of the data it was applied to.\nModel 2: Decision tree\nRationale for Selecting a Decision Tree\nWhile decision trees are not as complex as other modeling techniques, they do provide a foundation for more complex models, such as a random forest, which was the technique selected for our third model. As a non-parametric model, decision trees do not make any assumptions about the data, which is a positive feature in situations where it is challenging to assume a data distribution. As we randomly sampled 7% of the data, we cannot truly conclude that the score variable represents a true normal distribution. Therefore, a decision tree model provides a sensitive method to model the data to generalize to the complete data set in the case a normal distribution does not exist.\nModel Construction\nTo construct our decision tree model, we first tuned two of the three decision tree hyperparameters. Specifically, we tuned the cost_complexity and min_n hyperparameters to allow them to control the depth of the tree. Second, we utilized a regular grid with 10 possible values for the cost complexity and 5 possible values for the minimum n.\n\n688.82 sec elapsed\n\n# A tibble: 5 x 8\n  cost_complexity min_n .metric .estimator  mean     n std_err .config\n            <dbl> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1           0.001    40 rmse    standard    92.5    10   0.601 Model48\n2           0.001    30 rmse    standard    92.6    10   0.632 Model38\n3           0.001    21 rmse    standard    92.7    10   0.642 Model28\n4           0.001    11 rmse    standard    92.8    10   0.619 Model18\n5           0.001     2 rmse    standard    94.0    10   1.14  Model08\n396.967 sec elapsed\n# A tibble: 1 x 7\n  min_n .metric .estimator  mean     n std_err .config\n  <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1    35 rmse    standard    92.5    10   0.593 Model26\n\nFinalize the Decision Tree Model\n\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = 0.001\n  min_n = 35\n\nComputational engine: rpart \n# Resampling results\n# Monte Carlo cross-validation (0.75/0.25) with 1 resamples  \n# A tibble: 1 x 6\n  splits      id        .metrics    .notes    .predictions   .workflow\n  <list>      <chr>     <list>      <list>    <list>         <list>   \n1 <split [9.… train/te… <tibble [2… <tibble … <tibble [3,31… <workflo…\n# A tibble: 2 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      91.1  \n2 rsq     standard       0.365\n# A tibble: 3,314 x 3\n   .pred  .row score\n   <dbl> <int> <dbl>\n 1 2463.     2  2382\n 2 2711.     3  2533\n 3 2556.     4  2641\n 4 2580.    11  2429\n 5 2463.    18  2390\n 6 2403.    27  2246\n 7 2628.    29  2680\n 8 2580.    37  2625\n 9 2383.    41  2366\n10 2628.    42  2592\n# … with 3,304 more rows\n\nEvaluation of the Model\nAfter running the model, we ran collect_metrics() to evaluate the value of the RMSE and created a scatter plot to visualize the relationship between the mean RMSE values and cost_complexity values. The show_best() function revealed that the best RMSE values of the model ranged from 92.3-92.8. To reduce the value of the RMSE, we tuned our grid to limit the range of the min_n value from 10 to 40 and set the cost complexity to equal 0.001. After running the model with the updated tuned grid, the RMSE was slightly reduced to 92.29.\nModel 3: Random Forest\nRationale for Selecting a Random Forest\nRandom forests provide very strong “out of the box” model performance. Additionally, with a wide variety of predictive variables, random forests are adept to isolating the unique features of the data because a random selection of features is included in each split, which helps decorrelate the trees.\nModel Construction\nTo construct our random forest model in which the model was evaluated using OOB samples, the following steps were taken:\nWe ran a default random forest model, which generated a RMSE value of 92.7\nWe then ran a tuned random forest model, specifically tuning the mtry and min_n hyperparameters. The number of trees was set to 1000, and tune was set to 10.\n\n[1] 8\n90.905 sec elapsed\n# A tibble: 1 x 5\n  .metric .estimator  mean     n std_err\n  <chr>   <chr>      <dbl> <int>   <dbl>\n1 rmse    standard    92.7    10   0.670\n\n\n912.076 sec elapsed\n# A tibble: 5 x 8\n   mtry min_n .metric .estimator  mean     n std_err .config\n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1     6    40 rmse    standard    90.2    10   0.617 Model05\n2     8    20 rmse    standard    91.1    10   0.605 Model10\n3    16    30 rmse    standard    91.2    10   0.586 Model01\n4    21    33 rmse    standard    91.3    10   0.582 Model02\n5    25    25 rmse    standard    92.0    10   0.598 Model08\n\nEvaluation of the Model\nThe tuned random forest model yielded the best RMSE value of any model evaluated for our project. Dependent upon the random sample, the obtained RMSE ranged from 89.90 to 90.2.\nFinalize Model\nThe tuned random forest model generated the lowest RMSE value; therefore, it was determined to be the strongest model of all models evaluated for the project. To finalize this model, the following steps were taken:\nCreate a workflow with the tuned random forest model and recipe used in all models for the project.\nSelect the best model from the tuned random forest model.\nFit the model to the initial data split.\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ──────────────────────────────────────────────────────\n8 Recipe Steps\n\n● step_mutate()\n● step_novel()\n● step_unknown()\n● step_nzv()\n● step_medianimpute()\n● step_dummy()\n● step_nzv()\n● step_interact()\n\n── Model ─────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 6\n  trees = 1000\n  min_n = 40\n\nEngine-Specific Arguments:\n  num.threads = cores\n  importance = permutation\n  verbose = TRUE\n\nComputational engine: ranger \n11.571 sec elapsed\n[[1]]\n# A tibble: 2 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      88.7  \n2 rsq     standard       0.399\n\nMake Final Predictions\nTo generate final predictions, the finalized random forest workflow was fit to the training data (note: the trained data set was joined with the FRL and ethnicities data sets as outlined in the data description blog post).The ensuing fit was then applied to the test data set to generate final predictions.\n\n# A tibble: 3,314 x 1\n   .pred\n   <dbl>\n 1 2455.\n 2 2712.\n 3 2569.\n 4 2574.\n 5 2454.\n 6 2446.\n 7 2599.\n 8 2588.\n 9 2403.\n10 2555.\n# … with 3,304 more rows\n\n\n\n\n",
    "preview": "posts/2020-12-04-three-models/three-models_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-12-11T21:12:41-08:00",
    "input_file": "three-models.utf8.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Jim - Claire - Thuy",
        "url": "https://www.uoregon.edu/"
      }
    ],
    "date": "2020-12-04",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2020-12-09T19:52:40-08:00",
    "input_file": "welcome.utf8.md"
  }
]
