---
title: "Three models"
description: |
  Three models: fitting procedures and the results of model evaluation.
author:
  - name: Jim - Claire - Thuy
    url: https://www.uoregon.edu/
date: 12-04-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidymodels)
library(tune)
library(glmnet)
library(baguette)
library(parsnip)
library(doParallel)
library(vip)
library(pdp)
library(patchwork)
library(ranger)
library(future)
library(rio)
library(bit64)
```

## Data

```{r, include=FALSE}
# load the given data
set.seed(3000)
data <- read_csv(here::here("data", "train.csv")) %>% 
  select(-classification)

data <- dplyr::sample_frac(data, size = 0.07) #trying 7% 

# get freelunch data

frl <- import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
              setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>% 
  mutate(student_count = replace_na(student_count, 0))  %>% 
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>% 
  janitor::clean_names()  %>% 
  mutate(ncessch = as.double(ncessch))

stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv",
                     setclass = "tbl_df")  %>% 
  filter(state == "OR" & year == 1718)  %>% 
  count(ncessch, wt = n)  %>% 
  mutate(ncessch = as.double(ncessch))

frl <- left_join(frl, stu_counts)

frl <- frl %>% 
 mutate(prop_frl = free_lunch_qualified / n,
       prop_reduced_lunch = reduced_price_lunch_qualified / n)

# ethnicities

sheets <- readxl::excel_sheets(here::here("data",
"fallmembershipreport_20192020.xlsx"))

ode_schools <- readxl::read_xlsx(here::here("data",
"fallmembershipreport_20192020.xlsx"), sheet = sheets[4])

ethnicities <- ode_schools %>%
select(attnd_schl_inst_id = `Attending School ID`,
sch_name = `School Name`,
contains("%")) %>%
janitor::clean_names()
names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))

# join
data <- left_join(data, ethnicities)
data <- left_join(data, frl)
```

## Split and Resample 

```{r split, include=TRUE}
set.seed(3000)
data_split <- initial_split(data, strata = "score")

set.seed(3000)
train <- training(data_split)
test <- testing(data_split)

set.seed(3000)
data_cv <- vfold_cv(train, strata = "score")

```

## Recipe 
```{r recipe, include=TRUE}
# this from model 1
rec <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_novel(all_nominal()) %>%
  step_unknown(all_nominal()) %>%
  step_nzv(all_predictors()) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
  step_dummy(all_nominal(), -has_role("id vars")) %>%
  step_nzv(all_predictors()) %>% 
  step_interact(~lat:lon)

prep(rec)  
baked_train <- rec %>% prep() %>% bake(train)
baked_train
```

## Model 1: Linear Model 

### Rationale for Selecting a Linear Model 

Linear models assume a normal distribution within the data it is applied to. As demonstrated in the histogram, the score variable displays a normal distribution, which suggests a linear model can be fit to the data with both low variance and low bias. 

```{r, include=TRUE}
ggplot(data, aes(score)) +
  geom_histogram(fill = "#56B4E9",
                color = "white", 
                alpha = 0.9,
                bins = 25) +
  labs(x = "Score",
       y = "Number of Observations",
       title = "Distribution of Scores")
```

### Model Construction 

The three linear models presented in the course represent penalized regression models, which shrink the coefficients towards zero in order to reduce the model's overall variance. 

  1. Ridge Regression Model: Shrinks coefficients of correlated predictors toward each other, which is beneficial when the purpose of the model is to keep all predictors. 
  2. Lasso Regression Model: Selects one predictor to model the outcome while ignoring other predictors, which is beneficial to identifying the largest and most consistent predictor in data with may predictors.
  3. Elastic Net Regression Model: Combines the two types of penalties, which is better suited for multicollinearity. 
  
Instead of selecting one of the specific penalties to run our linear model, we opted to use cross-validation to find the optimal tuning parameters with a grid search. We specifically tuned the penalty and mixture parameters of the linear model. Our grid search utilized a regular grid with 10 possible values for the penalty and 5 possible values for the mixture, which produced 50 models per fold, resulting in a total of 500 models. 

```{r, include=TRUE} 
tictoc::tic()
final_mod <- linear_reg(penalty = tune(), 
                          mixture = tune())  %>% 
            set_engine("glmnet") %>%
            set_mode("regression")  

grid <- grid_regular(penalty(), mixture(), levels = c(10, 5))

final_mod_tuning <- tune_grid(final_mod, 
                              preprocessor = rec, 
                              resamples = data_cv, 
                              grid = grid, 
                              control = control_grid(verbose = TRUE))
tictoc::toc()

collect_metrics(final_mod_tuning) 

linear_mod_best <- final_mod_tuning %>% # 90.66920	
    show_best(metric = "rmse", n = 1)
linear_mod_best
```

### Evaluation of the Model 

The specific metric used to evaluate model performance was the RMSE. Utilzing the `show_best()` function, we isolated the best performing model, which obtained a RMSE value of approximately 90.5. As we randomly sampled 7% of the data, consecutive runs of the model generated RMSE values ranging from 90.4 - 90.6. Of note, initial iterations of the linear regression model were run exclusively on the combined *FRL* and training data, which resulted in a RMSE value of ~96.0. Expanding the data set to include the *ethnicities* and *ode_schools* and adding the interaction of `lat` and `lon` to the recipe decreased the RMSE to ~90.5. The strength of the linear model is likely a direct result of the normal distribution of the data it was applied to. 


#@ Model 2: Decision tree

### Rationale for Selecting a Decision Tree 

While decision trees are not as complex as other modeling techniques, they do provide a foundation for more complex models, such as a random forest, which was the technique selected for our third model. As a non-parametric model, decision trees do not make any assumptions about the data, which is a positive feature in situations where it is challenging to assume a data distribution. As we randomly sampled 7% of the data, we cannot truly conclude that the `score` variable represents a true normal distribution. Therefore, a decision tree model provides a sensitive method to model the data to generalize to the complete data set in the case a normal distribution does not exist.

### Model Construction 

To construct our decision tree model, we first tuned two of the three decision tree hyperparameters. Specifically, we tuned the `cost_complexity` and `min_n` hyperparameters to allow them to control the depth of the tree. Second, we utilized a regular  grid with 10 possible values for the cost complexity and 5 possible values for the minimum *n*. 

```{r, include=TRUE}
# 1. Tuning the cost complexity and minimum n 
tune_model <- decision_tree() %>% 
  set_mode("regression") %>% 
  set_engine("rpart") %>% 
  set_args(cost_complexity = tune(), 
           min_n = tune())

# 2. Tune your model with tune_grid

grd <- grid_regular(cost_complexity(), min_n(), levels = c(10, 5)) 
metrics_eval <- metric_set(rmse)

tictoc::tic()
tune_tree <- tune_grid(tune_model, 
                       rec, 
                       data_cv, 
                       grid = grd, 
                       metrics = metrics_eval)
tictoc::toc()
# 291.299 sec elapsed

# 4. Best Estimates 
collect_metrics(tune_tree) %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(cost_complexity, mean))+
  geom_point(aes(color = factor(min_n)))

# Check again, but remove highest one Add some jitter to avoid overlap
collect_metrics(tune_tree) %>%  
  filter(.metric == "rmse" & cost_complexity != 0.1) %>% 
  ggplot(aes(cost_complexity, mean))+
  geom_jitter(aes(color = factor(min_n)),
              height = 0, width = 0.01)

tune_tree %>% # 92.34186		
    show_best(metric = "rmse", n = 5)
# ==> this shows that we can get to rmse as small as 92.34186. Now try to tune grid again to make it smaller

grid_min_n <- tibble(min_n = 10:40)

dt_tune2 <- tune_model %>% 
  set_args(cost_complexity = 0.001)

tictoc::tic()
dt_tune_fit2 <- tune_grid(
  dt_tune2,
  preprocessor = rec,
  resamples = data_cv,
  grid = grid_min_n
)
tictoc::toc()

# look at metric again after re-tune
decision_tree_best <- show_best(dt_tune_fit2, metric = "rmse", n = 1) # Slight improvement 92.29472. Life is hard!
decision_tree_best
```


finalize the decision tree model 
```{r}
# set the final model parameters
best_params <- select_best(dt_tune_fit2, metric = "rmse")
final_mod_dt <- finalize_model(dt_tune2, best_params)
final_mod_dt

# use original `initial_split()` object
final_fit_dt <- last_fit(final_mod_dt, 
                         rec, 
                         data_split)
final_fit_dt
# view metrics

final_fit_dt$.metrics[[1]] # 95.6019768	

# prediction
predictions <- final_fit_dt$.predictions[[1]] # not so amazing though. The prediction is quite off. Well, it's a pandemic year :( 
predictions
```

### Evaluation of the Model 

After running the model, we ran `collect_metrics()` to evaluate the value of the RMSE and created a scatter plot to visualize the relationship between the mean RMSE values and cost_complexity values. The `show_best()` function revealed that the best RMSE values of the model ranged from 92.3-92.8. To reduce the value of the RMSE, we tuned our grid to limit the range of the min_n value from 10 to 40 and set the cost complexity to equal 0.001. After running the model with the updated tuned grid, the RMSE was slightly reduced to 92.29. 


## Model 3: Random Forest 

### Rationale for Selecting a Random Forest

Random forests provide very strong "out of the box" model performance. Additionally, with a wide variety of predictive variables, random forests are adept to isolating the unique features of the data because a random selection of features is included in each split, which helps decorrelate the trees. 

### Model Construction 

To construct our random forest model in which the model was evaluated using OOB samples, the following steps were taken: 

  1. Process the data by piping the recipe to the `prep()` and `bake()` functions.
  2. Write a function that fits the model for a given set of hyperparameters and returns a data frame that includes the hyperparameter values, RMSE, and model object.
  3. Fit model and evaluate values of hyperparameters to determine tuning procedure.
  4. Create grid to tune model evaluating `mtry` values of 5 to 15 and `min_n` values ranging from 5 to 25 by increments of 5. 
  5. Fit model and evaluate hyperparameters 
  6. Adjust grid to evaluate `mtry` values of 6 and 7 `min_n` values ranging from 20 to 25 by increments of 1. 
  7. Fit model and evaluate hyperparameters to identify lowest RMSE.
  8. Run model with the lowest RMSE value using tree values ranging from 500 to 1500 by increments of 100. 
  
# Model 3: Random Forest Model

## we first fit a default model

```{r, include=TRUE}
(cores <- parallel::detectCores())

rf_def_mod <-
  rand_forest() %>% 
  set_engine("ranger",
             num.threads = cores, #argument from {ranger}
             importance = "permutation", #argument from {ranger} 
             verbose = TRUE) %>% #argument from {ranger} 
  set_mode("regression")


tictoc::tic()
set.seed(3000)

rf_def_res <- fit_resamples(
  rf_def_mod,
  rec,
  data_cv,
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x)
)
tictoc::toc()

show_best(rf_def_res, "rmse")

```

## we then fit a tuned model

```{r}

rf_tune_mod <- rf_def_mod %>% 
  set_args(
    mtry = tune(),
    trees = 1000,
    min_n = tune()
  )

tictoc::tic()
set.seed(3000)
rf_tune_res <- tune_grid(
  rf_tune_mod,
  rec,
  data_cv,
  tune = 10,
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) extract_model(x))
)
tictoc::toc()

show_best(rf_tune_res, "rmse")
  
```

# Model 3 Summary

We used the same recipe and the same sub data set and fit two random forest models with the first being the default model and the second one tuned at 20 values. The default model gives us its best rmse value of 92.3 and the tuned model 


# finalize models
```{r}
rf_best <- select_best(rf_tune_res, metric = "rmse")

rf_final <- finalize_model(
  rf_tune_mod,
  rec,
  rf_best
)

rf_final

tictoc::tic()
set.seed(3000)
rf_res_final <- last_fit(rf_final,
                         split = data_cv)
tictoc::toc()
rf_res_final
```





