---
title: "Three models"
description: |
  Three models: fitting procedures and the results of model evaluation.
author:
  - name: Jim - Claire - Thuy
    url: https://www.uoregon.edu/
date: 12-04-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidymodels)
library(tune)
library(glmnet)
library(baguette)
library(parsnip)
library(doParallel)
library(vip)
library(pdp)
library(patchwork)
library(ranger)
library(future)
library(rio)
library(bit64)
```

## Data

```{r, include=FALSE}
# load the given data
set.seed(3000)
data <- read_csv(here::here("data", "train.csv")) %>% 
  select(-classification)

data <- dplyr::sample_frac(data, size = 0.07) #trying 7% 

# get freelunch data

frl <- import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
              setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>% 
  mutate(student_count = replace_na(student_count, 0))  %>% 
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>% 
  janitor::clean_names()  %>% 
  mutate(ncessch = as.double(ncessch))

stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv",
                     setclass = "tbl_df")  %>% 
  filter(state == "OR" & year == 1718)  %>% 
  count(ncessch, wt = n)  %>% 
  mutate(ncessch = as.double(ncessch))

frl <- left_join(frl, stu_counts)

frl <- frl %>% 
 mutate(prop_frl = free_lunch_qualified / n,
       prop_reduced_lunch = reduced_price_lunch_qualified / n)

# ethnicities

sheets <- readxl::excel_sheets(here::here("data",
"fallmembershipreport_20192020.xlsx"))

ode_schools <- readxl::read_xlsx(here::here("data",
"fallmembershipreport_20192020.xlsx"), sheet = sheets[4])

ethnicities <- ode_schools %>%
select(attnd_schl_inst_id = `Attending School ID`,
sch_name = `School Name`,
contains("%")) %>%
janitor::clean_names()
names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))

# join
data <- left_join(data, ethnicities)
data <- left_join(data, frl)
```

## Split and Resample 

```{r split, include=TRUE}
set.seed(3000)
data_split <- initial_split(data, strata = "score")

set.seed(3000)
train <- training(data_split)
test <- testing(data_split)

set.seed(3000)
data_cv <- vfold_cv(train, strata = "score")

```

## Recipe 
```{r recipe, include=TRUE}
# this from model 1
rec <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_novel(all_nominal()) %>%
  step_unknown(all_nominal()) %>%
  step_nzv(all_predictors()) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
  step_dummy(all_nominal(), -has_role("id vars")) %>%
  step_nzv(all_predictors()) %>% 
  step_interact(~lat:lon)

prep(rec)  
baked_train <- rec %>% prep() %>% bake(train)
baked_train
```

## Model 1: Linear Model 

### Rationale for Selecting a Linear Model 

Linear models assume a normal distribution within the data it is applied to. As demonstrated in the histogram, the score variable displays a normal distribution, which suggests a linear model can be fit to the data with both low variance and low bias. 

```{r, include=TRUE}
ggplot(data, aes(score)) +
  geom_histogram(fill = "#56B4E9",
                color = "white", 
                alpha = 0.9,
                bins = 25) +
  labs(x = "Score",
       y = "Number of Observations",
       title = "Distribution of Scores")
```

### Model Construction 

The three linear models presented in the course represent penalized regression models, which shrink the coefficients towards zero in order to reduce the model's overall variance. 

  1. Ridge Regression Model: Shrinks coefficients of correlated predictors toward each other, which is beneficial when the purpose of the model is to keep all predictors. 
  2. Lasso Regression Model: Selects one predictor to model the outcome while ignoring other predictors, which is beneficial to identifying the largest and most consistent predictor in data with may predictors.
  3. Elastic Net Regression Model: Combines the two types of penalties, which is better suited for multicollinearity. 
  
Instead of selecting one of the specific penalties to run our linear model, we opted to use cross-validation to find the optimal tuning parameters with a grid search. We specifically tuned the penalty and mixture parameters of the linear model. Our grid search utilized a regular grid with 10 possible values for the penalty and 5 possible values for the mixture, which produced 50 models per fold, resulting in a total of 500 models. 

```{r, include=TRUE} 
tictoc::tic()
final_mod <- linear_reg(penalty = tune(), 
                          mixture = tune())  %>% 
            set_engine("glmnet") %>%
            set_mode("regression")  

grid <- grid_regular(penalty(), mixture(), levels = c(10, 5))

final_mod_tuning <- tune_grid(final_mod, 
                              preprocessor = rec, 
                              resamples = data_cv, 
                              grid = grid, 
                              control = control_grid(verbose = TRUE))
tictoc::toc()

collect_metrics(final_mod_tuning) 

linear_mod_best <- final_mod_tuning %>% # 90.66920	
    show_best(metric = "rmse", n = 1)
linear_mod_best
```

### Evaluation of the Model 

The specific metric used to evaluate model performance was the RMSE. Utilzing the `show_best()` function, we isolated the best performing model, which obtained a RMSE value of approximately 90.5. As we randomly sampled 7% of the data, consecutive runs of the model generated RMSE values ranging from 90.4 - 90.6. Of note, initial iterations of the linear regression model were run exclusively on the combined *FRL* and training data, which resulted in a RMSE value of ~96.0. Expanding the data set to include the *ethnicities* and *ode_schools* and adding the interaction of `lat` and `lon` to the recipe decreased the RMSE to ~90.5. The strength of the linear model is likely a direct result of the normal distribution of the data it was applied to. 


#@ Model 2: Decision tree

### Rationale for Selecting a Decision Tree 

While decision trees are not as complex as other modeling techniques, they do provide a foundation for more complex models, such as a random forest, which was the technique selected for our third model. As a non-parametric model, decision trees do not make any assumptions about the data, which is a positive feature in situations where it is challenging to assume a data distribution. As we randomly sampled 7% of the data, we cannot truly conclude that the `score` variable represents a true normal distribution. Therefore, a decision tree model provides a sensitive method to model the data to generalize to the complete data set in the case a normal distribution does not exist.

### Model Construction 

To construct our decision tree model, we first tuned two of the three decision tree hyperparameters. Specifically, we tuned the `cost_complexity` and `min_n` hyperparameters to allow them to control the depth of the tree. Second, we utilized a regular  grid with 10 possible values for the cost complexity and 5 possible values for the minimum *n*. 

```{r, include=TRUE}
# 1. Tuning the cost complexity and minimum n 
tune_model <- decision_tree() %>% 
  set_mode("regression") %>% 
  set_engine("rpart") %>% 
  set_args(cost_complexity = tune(), 
           min_n = tune())

# 2. Tune your model with tune_grid

grd <- grid_regular(cost_complexity(), min_n(), levels = c(10, 5)) 
metrics_eval <- metric_set(rmse)

tictoc::tic()
tune_tree <- tune_grid(tune_model, 
                       rec, 
                       data_cv, 
                       grid = grd, 
                       metrics = metrics_eval)
tictoc::toc()
# 291.299 sec elapsed

# 4. Best Estimates 
collect_metrics(tune_tree) %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(cost_complexity, mean))+
  geom_point(aes(color = factor(min_n)))

# Check again, but remove highest one Add some jitter to avoid overlap
collect_metrics(tune_tree) %>%  
  filter(.metric == "rmse" & cost_complexity != 0.1) %>% 
  ggplot(aes(cost_complexity, mean))+
  geom_jitter(aes(color = factor(min_n)),
              height = 0, width = 0.01)

tune_tree %>% # 92.34186		
    show_best(metric = "rmse", n = 5)
# ==> this shows that we can get to rmse as small as 92.34186. Now try to tune grid again to make it smaller

grid_min_n <- tibble(min_n = 10:40)

dt_tune2 <- tune_model %>% 
  set_args(cost_complexity = 0.001)

tictoc::tic()
dt_tune_fit2 <- tune_grid(
  dt_tune2,
  preprocessor = rec,
  resamples = data_cv,
  grid = grid_min_n
)
tictoc::toc()

# look at metric again after re-tune
decision_tree_best <- show_best(dt_tune_fit2, metric = "rmse", n = 1) # Slight improvement 92.29472. Life is hard!
decision_tree_best
```


finalize the decision tree model 
```{r}
# set the final model parameters
best_params <- select_best(dt_tune_fit2, metric = "rmse")
final_mod_dt <- finalize_model(dt_tune2, best_params)
final_mod_dt

# use original `initial_split()` object
final_fit_dt <- last_fit(final_mod_dt, 
                         rec, 
                         data_split)
final_fit_dt
# view metrics

final_fit_dt$.metrics[[1]] # 95.6019768	

# prediction
predictions <- final_fit_dt$.predictions[[1]] # not so amazing though. The prediction is quite off. Well, it's a pandemic year :( 
predictions
```

### Evaluation of the Model 

After running the model, we ran `collect_metrics()` to evaluate the value of the RMSE and created a scatter plot to visualize the relationship between the mean RMSE values and cost_complexity values. The `show_best()` function revealed that the best RMSE values of the model ranged from 92.3-92.8. To reduce the value of the RMSE, we tuned our grid to limit the range of the min_n value from 10 to 40 and set the cost complexity to equal 0.001. After running the model with the updated tuned grid, the RMSE was slightly reduced to 92.29. 

# Model : Bagged Tree

```{r, include=TRUE}
# 1. set model 

bt_mod <- bag_tree() %>% 
  set_mode("regression") %>% 
  set_args(cost_complexity = tune(), min_n = tune()) %>% 
  set_engine("rpart", times = 50) # you will almost surely need more than 10


# fit to $k$ folds
tictoc::tic() 
bt_fit1 <- fit_resamples(bt_mod,
                       preprocessor = rec,
                       resamples = data_cv)
tictoc::toc() 

collect_metrics(bt_fit1)
# 2. Tune with tune_grid

tree_grid <- grid_max_entropy(cost_complexity(), min_n(), size = 10) 

plan(multisession) 
tictoc::tic()
bag_tune <- tune_grid(bag_mod_tune,
                      rec,
                      data_cv,
                      grid = tree_grid) 
tictoc::toc() # 311.272 sec elapsed

# 4. Best hyper parameters
select_best(bag_tune, "rmse")

# You should be doing more tuning than this. You need to actually evaluate your
# hyperparameters and see if there are trends and areas you need to do more
# detailed searches in. The space filling designs should just be a starting 
# point. They should give you a good approximation of hyperparamter combinations
# that work well, but you need to do more refined tuning  to find the best 
# combination
```

Finalize model 3
```{r}
final_bag_mod <- bag_mod_tune %>% 
  finalize_model(select_best(bag_tune, "rmse"))
```
Test fit model 3
```{r}
final_fit_bagtree <- last_fit(final_bag_mod, rec, data_split)
collect_metrics(final_fit_bagtree)
plan(sequential)
```



## Model 3: Random Forest 

### Rationale for Selecting a Random Forest

Random forests provide very strong "out of the box" model performance. Additionally, with a wide variety of predictive variables, random forests are adept to isolating the unique features of the data because a random selection of features is included in each split, which helps decorrelate the trees. 

### Model Construction 

To construct our random forest model in which the model was evaluated using OOB samples, the following steps were taken: 

  1. Process the data by piping the recipe to the `prep()` and `bake()` functions.
  2. Write a function that fits the model for a given set of hyperparameters and returns a data frame that includes the hyperparameter values, RMSE, and model object.
  3. Fit model and evaluate values of hyperparameters to determine tuning procedure.
  4. Create grid to tune model evaluating `mtry` values of 5 to 15 and `min_n` values ranging from 5 to 25 by increments of 5. 
  5. Fit model and evaluate hyperparameters 
  6. Adjust grid to evaluate `mtry` values of 6 and 7 `min_n` values ranging from 20 to 25 by increments of 1. 
  7. Fit model and evaluate hyperparameters to identify lowest RMSE.
  8. Run model with the lowest RMSE value using tree values ranging from 500 to 1500 by increments of 100. 
  


```{r, include=TRUE}
library(tictoc)
# Note: starting line 532 in Daniel's book, file "04-bagged-trees.Rmd"

#1. Prepare data

processed_reg <- rec %>% 
  prep() %>% 
  #bake(new_data = NULL) %>% #this can't run
  bake(new_data = train) %>% 
  select(-contains("id"), -ncessch, -tst_dt)

processed_reg

#2. A function for a set of hyperparameters 

rf_fit_reg <- function(tree_n, mtry, min_n) {
  rf_mod_reg <- rand_forest() %>% 
    set_mode("regression") %>% 
    set_engine("ranger") %>% 
    set_args(mtry = mtry,
             min_n = min_n,
             trees = tree_n)

  rf_fit <- fit(rf_mod_reg, 
                formula = score ~ .,
                data = processed_reg)
  
  # output RMSE
  tibble(rmse = sqrt(rf_fit$fit$prediction.error)) %>% 
    mutate(trees = tree_n, 
           mtry = mtry, 
           min_n = min_n,
           model = list(rf_fit))
}

```

```{r}
#3. Tuning

# number of preditors
ncol(processed_reg) - 1

# grid
grid_reg <- expand.grid(mtry = 5:15,
                        min_n = c(2, seq(5, 25, 5)))


tic()
rf_reg_fits <- map2_df(grid_reg$mtry, grid_reg$min_n, 
                       ~rf_fit_reg(1000, .x, .y))
toc()

rf_reg_fits %>% 
  arrange(rmse) # this take forever to run

# evaluate the hyperparameters

ggplot(rf_reg_fits, aes(mtry, rmse)) +
  geom_line() +
  facet_wrap(~min_n)

# evaluate values from 20 to 25 in increments of 1, while using both `mtry` values of 5 and 6.

grid_reg2 <- expand.grid(mtry = c(6, 7),
                         min_n = seq(20, 25, 1))

tic()
rf_reg_fits2 <- map2_df(grid_reg2$mtry, grid_reg2$min_n, 
                       ~rf_fit_reg(1000, .x, .y))
toc()

rf_reg_fits2 <- rf_reg_fits2 %>% 
  arrange(rmse)
rf_reg_fits2

#  Run model that had the lowest RMSE. Using trees from 500 to 1500 by increments of 100

tic()
rf_reg_ntrees <- map_df(seq(500, 1500, 100), 
                        ~rf_fit_reg(.x, 
                                    mtry = rf_reg_fits2$mtry[1], 
                                    min_n = rf_reg_fits2$min_n[1])
                        )
toc()
```

```{r}
#plot to find a point of stability
ggplot(rf_reg_ntrees, aes(trees, rmse)) +
  geom_line()
```

### Evaluation of the Model 



## Fit

```{r, include=TRUE}
# You have not setup a grid to search over. You have two parameters here to tune
# but you're moving to `fit_resamples()` instead of `tune_grid()`
# tictoc::tic()
# set.seed(3000)
# rf_def_res <- fit_resamples(
#   rf_wflow,
#   data_cv,
#   metrics = metrics_eval,
#   control = control_resamples(verbose = TRUE,
#                             save_pred = TRUE,
#                             extract = function(x) x))
# tictoc::toc()

# you should actually create a grid (probably start with a space filling 
# design and then go from there, as described above) but for now I'll just 
# do it with 10 values

tictoc::tic()
set.seed(3000)
rf_def_res <- tune_grid(
  rf_wflow,
  data_cv,
  metrics = metrics_eval,
  control = control_resamples(verbose = TRUE,
                            save_pred = TRUE,
                            extract = function(x) x))
tictoc::toc()

# Best Estimates 
show_best(rf_def_res, "rmse")

```




# Make Predictions 

```{r, include=TRUE}
## Read and join full test data 
full_test <- read_csv(here::here("data", "train.csv"))

full_test_join <- left_join(full_test, ethnicities)
```

## Apply Fit Function to Workflow and Full Training 

```{r, include=TRUE}
# You need to finalize your model before you can do the below because you have
# arguments that are set to `tune()`. They need actual values first.

# tictoc::tic()
# fit_rf_workflow <- fit(rf_wflow, train)
# tictoc::toc()

# fit_rf_workflow

# sqrt(fit_rf_workflow$fit$fit$fit$prediction.error)
```

```{r, include=TRUE}
# model_3_prediction <- predict(fit_rf_workflow, new_data = full_test_join)
# 
# head(model_3_prediction)
```

```{r, include=TRUE}
# pred_frame <- tibble(Id = full_test_join$id, Predict = model_3_prediction$.pred)
# 
# head(pred_frame)
```

```{r, include=TRUE}
# write_csv(pred_frame, "model_3_predictions.csv")
```

